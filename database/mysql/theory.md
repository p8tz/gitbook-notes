## 一. 范式

以一个例子说明范式的作用, 例子来源: [牛蛙的大杂烩](https://zhuanlan.zhihu.com/p/20028672)

第一范式就是说属性列不可再分, 下表已经满足第一范式

| 学号  | 姓名 | 系名     | 系主任 | 课名         | 分数 |
| ----- | ---- | -------- | ------ | ------------ | ---- |
| 95001 | 李勇 | 数学系   | 张清玫 | 复变函数     | 80   |
| 95001 | 李勇 | 数学系   | 张清玫 | 大学英语     | 90   |
| 95001 | 李勇 | 数学系   | 张清玫 | 数学分析     | 70   |
| 95002 | 刘晨 | 计算机系 | 刘逸   | 数据结构     | 65   |
| 95002 | 刘晨 | 计算机系 | 刘逸   | Java程序设计 | 90   |
| 95002 | 刘晨 | 计算机系 | 刘逸   | 数据库原理   | 65   |
| 95003 | 王敏 | 数学系   | 张清玫 | 复变函数     | 80   |
| 95003 | 王敏 | 数学系   | 张清玫 | 红楼梦赏析   | 85   |

### 问题

数据冗余: 出现大量重复数据

修改异常: 如果王敏转到计算机系, 需要修改三条记录, 容易出现数据不一致性

插入异常: 学校新建了一个系，但是暂时还没有招收任何学生，那么是无法将系名与系主任的数据单独地添加到数据表中去的(主属性不能为空)

删除异常: 假如将某个系中所有学生相关的记录都删除，那么所有系与系主任的数据也就随之消失了

### 基本概念及解决方案

#### 函数依赖

**若在一张表中，在属性（或属性组）X的值确定的情况下，必定能确定属性Y的值，那么就可以说Y函数依赖于X，写作$$X\stackrel{}{\rightarrow}Y$$**。也就是说，在数据表中，不存在任意两条记录，它们在X属性（或属性组）上的值相同，而在Y属性上的值不同。

#### 完全函数依赖

在一张表中，若 X → Y，且对于 X 的任何一个真子集（假如属性组 X 包含超过一个属性的话），X ' → Y 不成立，那么我们称 Y 对于 X **完全函数依赖**，记作 $$X\stackrel{F}{\rightarrow}Y$$

#### 部分函数依赖

假如 Y 函数依赖于 X，但同时 Y 并不完全函数依赖于 X，那么我们就称 Y 部分函数依赖于 X，记作$$X\stackrel{P}{\rightarrow}Y$$

#### 传递函数依赖

假如 Z 函数依赖于 Y，且 Y 函数依赖于 X （严格来说还有一个X 不包含于Y，且 Y 不函数依赖于Z的前提条件），那么我们就称 Z 传递函数依赖于 X ，记作$$X\stackrel{T}{\rightarrow}Y$$

#### 码

设 K 为某表中的一个属性或属性组，若除 K 之外的所有属性都完全函数依赖于 K（这个“完全”不要漏了），那么我们称 K 为**候选码**，简称为**码**。在实际中我们通常可以理解为：**假如当 K 确定的情况下，该表除 K 之外的所有属性的值也就随之确定，那么 K 就是码。**一张表中可以有超过一个码。（实际应用中为了方便，通常选择其中的一个码作为**主码**）

#### 主属性

包含在任意一个码中的属性称为主属性。

#### 非主属性

不包含在任何一个码中的属性称为非主属性。

#### 修正为第二范式

上表的码只有一个，就是**（学号、课名）**

所以主属性有两个：**学号** 与 **课名**, 其余的都是非主属性

对于**（学号，课名） → 姓名**，有 **学号 → 姓名**，存在非主属性 **姓名** 对码**（学号，课名）**的部分函数依赖。
对于**（学号，课名） → 系名**，有 **学号 → 系名**，存在非主属性 **系名** 对码**（学号，课名）**的部分函数依赖。
对于**（学号，课名） → 系主任**，有 **学号 → 系主任**，存在非主属性 **系主任** 对码**（学号，课名）**的部分函数依赖。

所以商标存在非主属性对于码的部分函数依赖，最高只符合1NF的要求，不符合2NF的要求。

一个满足第二范式的拆表如下

| 学号  | 课名         | 分数 |
| ----- | ------------ | ---- |
| 95001 | 复变函数     | 80   |
| 95001 | 大学英语     | 90   |
| 95001 | 数学分析     | 70   |
| 95002 | 数据结构     | 65   |
| 95002 | Java程序设计 | 90   |
| 95002 | 数据库原理   | 65   |
| 95003 | 复变函数     | 80   |
| 95003 | 红楼梦赏析   | 85   |

| 学号  | 姓名 | 系名     | 系主任 |
| ----- | ---- | -------- | ------ |
| 95001 | 李勇 | 数学系   | 张清玫 |
| 95002 | 刘晨 | 计算机系 | 刘逸   |
| 95003 | 王敏 | 数学系   | 张清玫 |

看看之前存在的问题有没有解决

数据冗余: 少了. ---- 有改进

修改异常: 王敏转到计算机系, 只需要修改一条记录. ---- 有改进

插入异常: 插入一个尚无学生的新系的信息。因为学生表的码是学号，不能为空，所以此操作不被允许. ---- 无改进

删除异常: 删除某个系中所有的学生记录该系的信息仍然全部丢失. ---- 无改进

所以还不够

#### 修正为第三范式

对于学生表，主码为**学号**，主属性为**学号**，非主属性为**姓名**、**系名**和**系主任**。因为 学号 → 系名，同时 系名 → 系主任，所以存在非主属性**系主任**对于码**学号**的传递函数依赖，所以**学生**表的设计，不符合3NF的要求。

最终一个满足第三范式的拆表如下

| 学号  | 课名         | 分数 |
| ----- | ------------ | ---- |
| 95001 | 复变函数     | 80   |
| 95001 | 大学英语     | 90   |
| 95001 | 数学分析     | 70   |
| 95002 | 数据结构     | 65   |
| 95002 | Java程序设计 | 90   |
| 95002 | 数据库原理   | 65   |
| 95003 | 复变函数     | 80   |
| 95003 | 红楼梦赏析   | 85   |

| 学号  | 姓名 | 系名     |
| ----- | ---- | -------- |
| 95001 | 李勇 | 数学系   |
| 95002 | 刘晨 | 计算机系 |
| 95003 | 王敏 | 数学系   |

| 系名     | 系主任 |
| -------- | ------ |
| 数学系   | 张清玫 |
| 计算机系 | 刘逸   |

再看一下之前的问题

删除异常: 删除某个系中所有的学生记录, 该系的信息不会丢失。---- 有改进

插入异常: 插入一个尚无学生的新系的信息。因为系表与学生表目前是独立的两张表，所以不影响。---- 有改进

数据冗余: 更少了。---- 有改进

### 总结

**第一范式**：属性不可分

**第二范式**：消除对码的部分函数依赖, 每个非主属性都完全函数依赖于码。体现为在使用联合主键的情况下, 不存在联合主键中部分属性就可以确定其它属性

**第三范式**：消除对码的传递函数依赖, 每个非主属性不传递函数依赖于码。体现为使用外键关联其它实体信息

## 二. 事务

### ACID

原子性：事务的操作, 要么全部完成, 要么全都不完成

一致性：事务执行前后的数据状态一致，这里一致可以认为是说符合逻辑，比如A给B转账100这个事务执行完后，成功应看到A-100，B+100, 不成功应看到A, B，不能出现中间其它状态

隔离性：并发访问时, 不同事务之间相互隔离, 不受影响，具体隔离效果取决于隔离级别

持久性：一旦事务提交，所产生的变化是持久化在硬盘中的, 服务器宕机重启后读到的数据也是变化之后的

### 隔离级别

- 读未提交：一个事务可以读另一个事务**写未提交**的数据，会产生脏读的问题
- 读已提交：一个事务只能读另一个事务**写已提交**的数据, **即事务所做的修改在提交之前对其它事务不可见**. 可以解决脏读的问题，会产生不可重复读的问题
- 可重复读：一个事务内对同一条记录的读取总是一致的, 解决不可重复读的问题，会产生幻读的问题
- 可串行化：强制事务串行执行, 不会出现任何并发问题

|          | 丢失修改 | 脏读 | 不可重复读 | 幻读 |
| :------: | :------: | :--: | :--------: | :--: |
| 读未提交 |    Y     |  N   |     N      |  N   |
| 读已提交 |    Y     |  Y   |     N      |  N   |
| 可重复读 |    Y     |  Y   |     Y      |  N   |
| 可串行化 |    Y     |  Y   |     Y      |  Y   |

### 并发问题

1. 丢失修改: 两个事务先后对同一块数据进行**写**的操作, 导致先写入的事务修改结果丢失. mysql中不允许两个事务同时**写**一块数据, 因此不会出现这样的情况
2. 脏读：事务 A 读取了事务B更新的数据，然后B回滚操作，那么A读取到的就是脏数据
3. 不可重复读：事务 A 多次读取同一数据，事务 B 在事务A多次读取的过程中，对数据作了修改并提交，导致事务A多次读取同一数据时，结果不一致。
4. 幻读：幻读本质上也属于不可重复读的情况，事务 A 读取某个范围的数据，事务 B 在这个范围内插入新的数据，A 再次读取这个范围的数据，此时读取的结果和和第一次读取的结果不同。

​		小结：不可重复读侧重于修改或删除，幻读侧重于新增。解决不可重复读的问题只需锁住对应的行(因为是对已存在的行操作)，解决幻读需要锁表(不知道哪里会插来数据)

### AUTOCOMMIT

mysql默认开启自动提交事务, 每条语句默认为一个事务. 如果显示开启start transaction则会关闭自动提交, 遇到COMMIT再提交

通过下面设置默认事务是否开启, 0关闭, 1开启

```mysql
SET AUTOCOMMIT = 0 | 1
```

## 三. 锁

### 锁粒度

行锁：开销大，加锁慢；会出现死锁，并发度高

表锁：开销小，加锁快；不会出现死锁，并发度低

### 读写锁

共享锁（读锁）：加了 S 锁后，别的事务仍然可以加 S 锁，但不能加 X 锁，加了锁的事务只能对数据进行读操作

排他锁（写锁）：加了 X 锁后，别的事务不能对其加任何锁，只有加了锁的事务可以操作数据

### 意向锁

由于表锁和行锁的范围不同，可能会产生冲突。在加表锁之前需要逐行判断有没有行锁，确认没有行锁才能加表锁，这样的判断方式肯定效率低下，由此出现了意向锁来解决行锁和表锁冲突的问题。意向锁分为读意向锁和写意向锁，二者都是表锁。当事务需要加读锁或写锁时，首先要加意向锁。这样加意向锁之前判断该表有没有意向锁就可以了

意向锁之间不会产生冲突，只会阻塞表级读锁和写锁，意向锁也不会和行锁冲突，只是相当于一个锁标志，告诉要加表锁的事务，我这个表有行锁，现在不能加表锁

### 补充

意向锁是 InnoDB 自动加的，不需用户干预。对于 `UPDATE`、``DELETE` 和 `INSERT` 语句，InnoDB会自动给涉及的数据集加排他锁（X)；对于普通`SELECT`语句，InnoDB 不会加任何锁；可以通过以下语句显示给记录集加共享锁或排他锁

```mysql
SELECT ... LOCK IN SHARE MODE # 共享锁(S)
SELECT ... FOR UPDATE 		  # 排他锁(X)
```

### 封锁协议

#### 一级封锁协议

写之前加 X 锁, 事务结束释放 X 锁. 实现了读未提交

> 因为不能对同一数据同时修改, 因此不会有丢失修改的问题

#### 二级封锁协议

在一级基础上, 读之前加 S 锁, 读完后释放 S 锁. 实现了读已提交

>  因为事务 A 修改数据时, 事务 B 不能加 S 锁, 也就不能读, 因此不会有脏读的问题

#### 三级封锁协议

在一级基础上, 读之前加 S 锁, 事务结束释放 S 锁. 实现了可重复读

> 因为事务 A 多次读数据的时候, 只要事务没结束, 就不能释放 S 锁, 其它释放也就不能加 X 锁修改, 因此不会有可重复读的问题

#### 两段锁协议

两段锁协议是一种能够实现并发调度可串行化的封锁协议. 两段锁规定:

1. 对数据 D 读之前加 S 锁, 写之前加 X 锁
2. 一旦在 D 上有锁释放, 则不能再给 D 加锁, 直到锁全部释放完

在遵循两段锁协议的事务中, 明显地可分为两个阶段: 第一阶段是锁逐步增加阶段, 第二阶段是锁逐步释放阶段

实现了可串行化

## 四. MVCC

> 原文链接: [CS-Notes](https://cyc2018.github.io/CS-Notes/#/notes/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F%E5%8E%9F%E7%90%86?id=_1-%e5%bf%ab%e7%85%a7%e8%af%bb) [YoungChen](https://zhuanlan.zhihu.com/p/64576887)

多版本并发控制（Multi-Version Concurrency Control, MVCC）是 MySQL 的 InnoDB 存储引擎实现隔离级别的一种具体方式，用于实现`RC`和`RR`这两种隔离级别。

### 基本思想

在多级封锁协议中，加锁能解决多个事务同时执行时出现的并发一致性问题。在实际场景中读操作往往多于写操作，因此又引入了读写锁来避免不必要的加锁操作，例如读和读没有互斥关系。读写锁中读和写操作仍然是互斥的，而 MVCC 利用了多版本的思想，写操作更新最新的版本快照，而读操作去读旧版本快照，没有互斥关系，这一点和 CopyOnWrite 类似。

在 MVCC 中事务的修改操作（DELETE、INSERT、UPDATE）会为数据行新增一个版本快照。

脏读和不可重复读最根本的原因是事务读取到其它事务未提交的修改。在事务进行读取操作时，为了解决脏读和不可重复读问题，MVCC 规定只能读取已经提交的快照。当然一个事务可以读取自身未提交的快照，这不算是脏读。

### 当前读

对数据库进行修改的操作（`INSERT`、`UPDATE`、`DELETE`）需要进行加锁操作，从而读取最新的数据, 不允许其它事务修改当前记录, 这就是当前读. `SELECT`可以强制加锁进行当前读

```mysql
SELECT * FROM ... FOR UPDATE;  		  # X
SELECT * FROM ... LOCK IN SHARE MODE; # S
```

### 快照读

不加锁的`SELECT`就是快照读

```mysql
SELECT * FROM ...;
```

### 版本号

系统版本号 `SYS_ID`：是一个递增的数字，每开始一个新的事务，系统版本号就会自动递增。

事务版本号 `TRX_ID`：事务开始时的系统版本号。

### 隐式字段

每行记录除了我们自定义的字段外，还有数据库隐式定义的一些字段

![image-20201023222507671](https://gitee.com/p8t/picbed/raw/master/imgs/20201023222508.png)

`TRX_ID`
6字节，记录最近更新这条行记录的事务 ID

`ROLL_PTR`
7字节，回滚指针，指向这条记录的上一个版本. `InnoDB` 便是通过这个指针找到之前版本的数据。该行记录上的所有旧版本，在 `undo log` 中都通过链表的形式组织。

`ROW_ID`
6字节，隐含的自增ID（隐藏主键），如果数据表没有主键，可以作为聚簇索引

### undo log

MVCC 的多版本指的就是多个版本的快照，快照存储在`undo log`中，该日志通过回滚指针 `ROLL_PTR` 把一个数据行的所有快照连接起来。 **`SELECT`数据的过程就是查找版本链的过程**

注意下图只有旧数据在`undo log`中.

![image-20201023223341390](https://gitee.com/p8t/picbed/raw/master/imgs/20201023223342.png)

### Read View

在 `RU` 隔离级别下，直接读取版本的最新记录就可以，对于 `SERIALIZABLE` 隔离级别，则是通过加锁互斥来访问数据，不需要`MVCC`。因此`MVCC`运行在`RC`和`RR`这两个隔离级别下，当`InnoDB`隔离级别设置为其中之一时，在`SELECT`数据时就会用到版本链

#### RR 下的 ReadView 生成

在`RR`隔离级别下，每个事务`touch first read`时（本质上就是执行第一个 `SELECT` 语句时，后续所有的`SELECT`都是复用这个`ReadView`，其它 `UPDATE`, `DELETE`, `INSERT` 语句和一致性读 `snapshot` 的建立没有关系），会将当前系统中的所有的活跃事务(未提交事务)拷贝到一个列表生成`ReadView`。

#### RC 下的 ReadView 生成

在`RC`隔离级别下，每个`SELECT`语句开始时，都会重新将当前系统中的所有的活跃事务拷贝到一个列表生成`ReadView`。和`RR`区别就在于生成`ReadView`的时间点不同，一个是事务之后第一个`SELECT`语句开始、一个是事务中每条`SELECT`语句开始。

#### 执行流程

`ReadView` 中是当前活跃的事务 `ID` 列表，称之为 `m_ids`，其中最小值为 `up_limit_id`，最大值为 `low_limit_id`，事务 `ID` 是事务开启时 `InnoDB` 分配的，其大小决定了事务开启的先后顺序，因此我们可以通过 `ID` 的大小关系来决定版本记录的可见性，具体判断流程如下：

1. 如果被访问版本的 `trx_id` 小于 `m_ids` 中的最小值 `up_limit_id`，说明生成该版本的事务在 `ReadView` 生成前就已经提交了，所以该版本可以被当前事务访问。
2. 如果被访问版本的 `trx_id` 大于 `m_ids` 列表中的最大值 `low_limit_id`，说明生成该版本的事务在生成 `ReadView` 后才生成，所以该版本不可以被当前事务访问。需要根据 `Undo Log` 链找到前一个版本，然后根据该版本的 DB_TRX_ID 重新判断可见性。
3. 如果被访问版本的 `trx_id` 属性值在 `m_ids` 列表中最大值和最小值之间（包含）
   1. 在`RC`隔离级别下, 就需要判断一下 `trx_id` 的值是不是在 `m_ids` 列表中。如果在，说明创建 `ReadView` 时生成该版本所属事务还是活跃的，因此该版本不可以被访问，需要查找 Undo Log 链得到上一个版本，然后根据该版本的 `DB_TRX_ID` 再从头计算一次可见性；如果不在，说明创建 `ReadView` 时生成该版本的事务已经被提交，该版本可以被访问。
   2. 在`RR`隔离级别下, 都不可访问(因为, 可能会有别的事务对其作了修改), 需要查找 Undo Log 链得到上一个版本，然后根据该版本的 `DB_TRX_ID` 再从头计算一次可见性
4. 此时经过一系列判断我们已经得到了这条记录相对 `ReadView` 来说的可见结果。此时，如果这条记录的 `delete_flag` 为 `true`，说明这条记录已被删除，不返回。否则说明此记录可以安全返回给客户端。

### 总结

![image-20201025231222210](https://gitee.com/p8t/picbed/raw/master/imgs/20201025231223.png)

0. `ReadView`生成的就是图中蓝色部分那一段, 正在活跃的事务

1. 在`RR`隔离级别下, 一个事务对同一条记录只有第一次读产生`ReadView`, 然后通过查找版本链`(undo log)`找到符合要求的记录, 以后每次读取的数据都是这个, 因此前后读的数据是一致的

> 对应图中: `MIN_TRX_ID`之前的可见, 此后的都不可见, 遇到这种情况, 查找版本链, 找到之前的为止

2. 在`RC`隔离级别下, 一个事务对同一条记录每次读都会产生一个`ReadView`, 每次都通过查找版本链`(undo log)`找到符合要求的记录, 因此前后读的数据是可能不一致的

> 对应图中: `MIN_TRX_ID`之前的可见, `MIN_TRX_ID`之后的不可见. 对于中间那一段, 看读的记录的`TRX_ID`是否在`ReadView`中出现, 如果出现说明事务仍在进行, 还未提交, 不可见. 否则可见

3. `undo log`提供版本链, 不同的`ReadView`生成时机以及对中间段事务不同的处理方式实现了`RC`和`RR`
4. 凡是遇到不可见的, 就去查找版本链

在`RR`下普通的`SELECT`不会有幻读的问题. 而`SELECT ... FOR UPDATE`则会有幻读的问题, 因为这是当前读, 可以读到最新已经提交的数据. 其它的当前读(`UPDATE DELETE INSERT`)通过`Next-Key Lock`解决幻读问题. 

因此在`RR`隔离级别下, 基本解决了幻读的问题, 但`SELECT ... FOR UPDATE`依然能产生幻读

## 五. Next-Key Lock

`Next-Key Lock` 是 MySQL 的 InnoDB 存储引擎的一种锁实现。

MVCC 不能解决幻影读问题，`Next-Key Lock` 就是为了解决这个问题而存在的。**在`RR`隔离级别下**，使用 `MVCC + Next-Key Lock` 可以解决幻读问题。

`Record Lock`（行锁），`Gap Lock`，`Next-Key Lock`都属于**排他锁**。

### Record Lock

锁定一个记录上的索引，而不是记录本身。

如果查询用的是主键索引则直接锁, 如果查询用的是普通索引则会先锁普通索引, 再把对应的主键索引也锁住

### Gap Lock

锁定索引之间的间隙，但是不包含索引本身。例如当一个事务执行以下语句，其它事务就不能在 t.c 中插入 15。

```mysql
SELECT c FROM t WHERE c BETWEEN 10 and 20 FOR UPDATE;
```

`Gap Lock`在InnoDB的唯一作用就是防止其他事务的插入操作，以防止**幻读**的发生。

### Next-Key Lock

它是`Record Lock`和`Gap Lock`的结合，不仅锁定一个记录上的索引，也锁定索引之间的间隙。它锁定一个左开右闭区间

### 总结

![image-20201026154341033](https://gitee.com/p8t/picbed/raw/master/imgs/20201026154342.png)

- 不通过索引查询, 会锁表
- InnoDB中的行锁指`Record Lock`(另外两个也是行锁), 锁的不是记录, 而是索引
- 使用主键索引, 则锁主键索引; 使用非主键索引, 先锁非主键索引, 再锁主键索引
- `RR`隔离级别下使用`Next-Key Lock`解决幻读问题
- `MVCC`保证快照读不会产生幻读
- `Next-Key Lock`保证当前读不会产生幻读, 除了`SELECT ... FOR UPDATE`

## 六. 索引

### B树和B+树

**数据结构**

一颗m叉的B树特性如下：

- 树中每个节点最多包含m个孩子。
- 除根节点与叶子节点外，每个节点至少有`[ceil(m/2)]`个孩子。
- 若根节点不是叶子节点，则至少有两个孩子。
- 所有的叶子节点都在同一层。
- 每个非叶子节点由`n`个`key`与`n+1`个指针组成，其中`[ceil(m/2)-1] <= n <= m-1`

![image-20201022203816647](https://gitee.com/p8t/picbed/raw/master/imgs/20201022203817.png)

B+Tree为BTree的变种，区别为：

- n叉`B+Tree`最多含有`n`个`key`，而BTree最多含有`n-1`个key。
- `B+Tree`的叶子节点保存所有的key信息，依`key`大小顺序排列。
- 所有的非叶子节点都可以看作是`key`的索引部分。

![image-20201022211908229](https://gitee.com/p8t/picbed/raw/master/imgs/20201022211909.png)

**对比**

二叉搜索树 / AVL树 / 红黑树: 每个节点只存放一个索引, 树较高, 不利于磁盘IO

B树: 每个节点既存放索引也存放数据, 相比于红黑树提高了单页索引量, 但是因为数据和索引放在一起, 数据会挤占索引的空间, 可以进一步分离. 此外, 对于范围查找不方便

B+树: 只有叶子节点存放数据，其它的节点只存放索引，使得一个页可容纳的索引数量大大增加，并且每个叶子节点之间以链表的形式连接起来，极大提高了范围查找的效率

### MySQL中的索引

B+Tree索引：最常见的索引, 不需要全表扫描, 后面说的都是基于这种索引

Hash索引：基于哈希表实现，数组加链表，适用于精确查找，做不到范围查询, memory默认使用

R-tree 索引（空间数据索引）：MyISAM的一个特殊索引类型，可以用作地理数据存储。空间索引会从所有维度来索引数据, 可以有效地使用任意维度来组合查询。

Full-text （全文索引） ：全文索引也是MyISAM的一个特殊索引类型，它查找的是文本中的关键词，而不是直接比较索引中的值。全文索引适用于MATCH AGAINST操作，而不是普通的WHERE条件操作。InnoDB从`MySQL5.6.4`版本开始支持全文索引。

### B+Tree索引分类

主键索引：唯一，不允许`null`值

唯一索引：唯一，只允许一个`null`值

普通索引：无限制

联合索引：多个属性组成一个索引

### 聚簇索引和非聚簇索引

聚簇索引: 索引和真正的数据放在一块, 也就是索引叶子节点存储了该行的数据

非聚簇索引: 叶子节点存的不是真正的数据, 可以是数据的起始地址, 也可以是主键索引的值

在InnoDB中, 每个表**有且仅有**一个聚簇索引，建表时会自动为`PRIMARY KEY`创建，如果没有主键则会找一个`NOT NULL UNIQUE`作为聚簇索引，如果还没有，则使用6字节的`row_id`作为聚簇索引. 
除了聚簇索引之外的都是非聚簇索引, 只有聚簇索引的叶子存储的是数据，其它索引存的都是主键值，然后通过主键再次查询得到数据，因此查询非聚簇索引会有两次查询过程，第二次称为**回表**

在MyISAM中, 索引叶子节点存的是数据的地址, 因此是非聚簇索引

### 概念

**单列索引**: 以单个字段建立的索引

**复合索引**: 以多个字段建立的索引, 又叫组合索引 / 联合索引

**前缀索引**: 对于比较长的字符类型的索引, 可以指定以前多少位来建立索引, 建立时应当注意: 索引部分尽可能的不重复, 否则效率大大降低

> 假设有一张表, 其中一个索引数据类型为100个字符, 截取前5个, 发现会有大量重复, 截取前7个, 重复大大降低, 则应当以前7个字符建立索引, 而不是以整个字段(100个字符)或前5个字符建立索引

#### 全值匹配

对于联合索引, 如果查询条件包含了其中每一个属性且均为AND连接的精确匹配, 则称为全值匹配

> 有联合索引 `id, name, age`, 对于SQL语句: `SELECT * FROM stu WHERE id=1 AND name='jljxvg' AND age=18`, 可以称为其索引的全值匹配

#### 最左前缀匹配

对于联合索引，查询时会从最左边开始匹配 (书写顺序不重要)，且中间不能缺, 否则从缺的开始都不算入索引

> 有联合索引 `id, name, age`, 则`id` / `id, name`/ `id, name, age`都是合法索引, 查询的时候都会走对应的索引; 而`id, age`虽然也会走索引, 但只走`id`, 相当于索引就是`id`; 对于`name, age`从左边开始一个都匹配不到, 因此不会走索引,执行全表查询
>
> 个人猜想原因: 联合索引越靠左边的属性, 排序的优先级越高, 如果跳过第二列则找不到这一列的范围, 就无法进行第三列的匹配. 而不跳过的话, 只需要把已经匹配的数据返回即可

#### 回表

查询非主键(聚簇)索引时，第一次查询得到主键值，第二次查询才得到数据，其中第二次查询过程称为回表

#### 索引覆盖

查询非主键索引不一定需要回表，只要保证查询的数据包含于联合索引即可

> 因为查询的数据全都在索引里面

#### 索引下推

如果没有ICP（Index Condition Pushdown），对于联合索引，查询的时候会先把满足第一个条件的主键查出来，然后回表，对数据在server层进行剩余条件筛选

有了ICP之后，查询的时候不会忽略剩余条件，直接在存储引擎进行筛选

### 索引优化

**结构修改**: 不要对频繁改动的列建立索引, 因此每次改动都会导致索引更新, 甚至重构, 索引越多, 时间越长

**覆盖索引**: 尽量使用覆盖索引, 这样不会进行回表查询(针对InnoDB), 提高效率

**全值匹配**：索引一定生效

**最左前缀匹配**：从缺的列开始, 后面的索引失效,. 因此建立联合索引应该把常用索引放左边

**范围查询**：范围查询之后的索引失效

> 有联合索引 `id, score`, age, 对于SQL语句: `SELECT * FROM stu WHERE id=1 AND score>60 AND age=18`, 走的索引为`id, score`

**字段运算**：对索引字段运算会导致索引失效

> 有索引name, 查询name第二个字为风的数据: `SELECT * FROM stu WHERE substring(name, 2, 1)='风'`, 此时索引失效

**字符串不加单引号**：索引失效，原因是优化器会加上单引号，相当于给索引字段进行了运算. 因此对于字符串一定要显示加上引号

### 索引失效

- 不满足最左前缀法的索引失效

- 范围查询之后的索引失效

- 对索引字段进行运算, 索引失效

- 字符串不加引号，索引失效

- 用`OR`连接的条件, 只要有一边没有建立索引, 那么索引失效

> 有索引`id`, 对于SQL语句: `SELECT * FROM stu WHERE id=1 OR name='xxx'`. 如果id走索引, 那么筛选name的时候还是要全表扫描, 因此没必要走索引, 直接全表扫描, 即索引失效

- 以%开头的like模糊查询，索引失效，可以通过覆盖索引来使索引生效, 但是这个生效并不是利用索引来定位数据, 而是遍历索引树, 因为数据都在树上. 
  想要真正的解决可以建立一个冗余列, 把数据反转查询

> 有索引id, name
> 对于 `SELECT * FROM stu WHERE name LIKE '风%'`索引生效 
> 对于 `SELECT * FROM stu WHERE name LIKE '%风'`则索引失效
> 对于 `SELECT id FROM stu WHERE name LIKE '%风'`索引生效, 但并不是利用索引检索数据, 所以查询速度还是慢
> 对于 `SELECT * FROM stu WHERE number LIKE '%123', 可以建立冗余列, 数据为number的反转, 然后 `SELECT * FROM stu WHERE rev_number LIKE '321%',

- 如果MySQL评估全表扫描比索引快，则走全表扫描，索引失效

> 比如一张表有10条记录, 其中9条记录索引score均为90, 则查询`score=90`时, 不走索引

- IS  NULL ， IS NOT NULL  有时索引失效，原因和上面一样

- IN, NOT IN  范围较小走索引，超出范围不走索引

> 有索引`id`
> 对于 `SELECT * FROM stu WHERE id IN (1, 2, 3)`走索引
> 对于 `SELECT * FROM stu WHERE id IN (1, 2, 3, 4, 5, 6)`不走索引
> 对于 `SELECT * FROM stu WHERE id NOT IN (1, 2, 3)`不走索引

## 七. 查询优化

### explain

| Column          | Meaning                                        |
| :-------------- | :--------------------------------------------- |
| `id`            | The `SELECT` identifier                        |
| `select_type`   | The `SELECT` type                              |
| `table`         | The table for the output row                   |
| `partitions`    | The matching partitions                        |
| `type`          | The join type                                  |
| `possible_keys` | The possible indexes to choose                 |
| `key`           | The index actually chosen                      |
| `key_len`       | The length of the chosen key                   |
| `ref`           | The columns compared to the index              |
| `rows`          | Estimate of rows to be examined                |
| `filtered`      | Percentage of rows filtered by table condition |
| `Extra`         | Additional information                         |

`id` 

- `id`相同, 从上往下执行
- `id`不同, `id`越大, 越先执行

`select_type`

| Value          | Meaning                                                      |
| :------------- | :----------------------------------------------------------- |
| `SIMPLE`       | 简单的`SELECT`查询，查询中不包含子查询或者UNION              |
| `PRIMARY`      | 查询中若包含任何复杂的子查询，最外层查询标记为该标识         |
| `SUBQUERY`     | 在`SELECT` 或 `WHERE` 列表中包含的子查询                     |
| `DERIVED`      | 在`FROM` 列表中包含的子查询                                  |
| `UNION`        | 若第二个SELECT出现在UNION之后，则标记为UNION ； 若UNION包含在FROM子句的子查询中，外层SELECT将被标记为 ： DERIVED |
| `UNION RESULT` | 从UNION表获取结果的SELECT                                    |

`table`

显示了对应行正在访问哪个表

`type`

访问类型, 即MySQL决定如何查找表中的行。下表执行效率由高到低

| type   | 含义                                                         |
| ------ | ------------------------------------------------------------ |
| NULL   | MySQL不访问任何表，索引，直接返回结果                        |
| system | 表只有一行记录(等于系统表)，这是const类型的特例，一般不会出现 |
| const  | 表示通过索引一次就找到了，const 用于比较primary key 或者 unique 索引。因为只匹配一行数据，所以很快。如将主键置于where列表中，MySQL 就能将该查询转换为一个常亮。const于将 "主键" 或 "唯一" 索引的所有部分与常量值进行比较 |
| eq_ref | 类似ref，区别在于使用的是唯一索引，使用主键的关联查询，关联查询出的记录只有一条。常见于主键或唯一索引扫描 |
| ref    | 非唯一性索引扫描，返回匹配某个单独值的所有行。本质上也是一种索引访问，返回所有匹配某个单独值的所有行（多个） |
| range  | 只检索给定返回的行，使用一个索引来选择行。 where 之后出现 between ， < , > , in 等操作。 |
| index  | index 与 ALL的区别为  index 类型只是遍历了索引树， 通常比ALL 快， ALL 是遍历数据文件。 |
| all    | 将遍历全表以找到匹配的行                                     |

一般来说， 我们需要保证查询至少达到 range 级别， 最好达到ref 

`possible_keys`: 可能使用的索引

 `key ` : 真正使用的索引

`key_len`: MySQL在索引里使用的字节数, key_len列显示了在索引字段中可能的最大长度，而不是表中数据 使用的实际字节数。

`ref`: 显示了之前的表在key列记录的索引中查找值所用的列或常量

`rows`: 扫描的行数

`Extra`: 额外信息

Using index

使用了覆盖索引，访问索引树, 避免了访问表。

Using index condition

命中索引，但不是所有的列数据都在索引树上，还需要访问实际的行记录, 即回表

Using where

使用where条件进行过滤

Using temporary

需要建立临时表(temporary table)来暂存中间结果。 

常见于group by和order by

Using filesort

对结果使用一个外部索引排序，而不是按索引次序从表里读取行。

在一个没有建立索引的列上进行了order by，就会触发filesort





## 八. 存储引擎

### InnoDB

InnoDB是MySQL的默认事务型引擎

采用`MVCC`来支持高并发，并且实现了四个标准的隔离级别。默认级别为`RR`，并且通过间隙锁（`MVCC + Next-Key Lock`）策略防止幻读的出现。间隙锁使得InnoDB不仅仅锁定查询涉及的行，还会对索引中的间隙进行锁定，以防止幻影行的插入。

InnoDB内部做了很多优化，包括从磁盘读取数据时采用的可预测性预读，能够自动在内存中创建hash索引以加速读操作的自适应哈希索引（`adaptive hash index`），以及能够加速插入操作的插入缓冲区（`insert buffer`）等

作为事务型的存储引擎，InnoDB通过一些机制和工具支持真正的热备份。MySQL的其他存储引擎不支持热备份， 要获取一致性视图需要停止对所有表的写入，而在读写混合场景中，停止写入可能也意味着停止读取。

### MyISAM

在MySQL 5.1及之前的版本，MyISAM是默认的存储引擎。MyISAM提供了大量的特性，包括全文索引、压缩、空间函数（GIS） 等，但MyISAM**不支持事务行级锁**，而且有一个毫无疑问的缺陷就是崩溃后无法安全恢复。尽管MyISAM引擎不支持事务、不支持崩溃后的安全恢复，但它绝不是一无是处的。对于只读的数据，或者表比较小、可以忍受修复（repair）操作，则依然可以继续使用MyISAM

**加锁与并发:** MyISAM对整张表加锁，而不是针对行。读取时会对需要读到的所有表加共享锁，写入时则对表加排他锁。但是在表有读取查询的同时，也可以往表中插入新的记录（这被称为并发插入，CONCURRENT INSERT）。

**修复: **对于MyISAM表，MySQL可以手工或者自动执行检查和修复操作，但这里说的修复和事务恢复以及崩溃恢复是不同的概念。执行表的修复可能导致一些数据丢失，而且修复操作是非常慢的。

**延迟更新索引键（Delayed Key Write）: ** 创建MyISAM表的时候，如果指定了DELAY_KEY_WRITE选项， 在每次修改执行完成时，不会立刻将修改的索引数据写入磁盘，而 是会写到内存中的键缓冲区（in-memory key buffer），只有在清理键缓冲区或者关闭表的时候才会将对应的索引块写入到磁盘。这种方式可以极大地提升写入性能，但是在数据库或者主机崩溃时会造成索引损坏，需要执行修复操作。延迟更新索引键的特性，可以在全局设置，也可以为单个表设置。

### 对比

- 事务: InnoDB支持事务, MyISAM不支持
- 并发: InnoDB支持行锁, MyISAM不支持
- 外键: InnoDB支持外键, MyISAM不支持
- 索引: InnoDB主键是聚簇索引，MyISAM是非聚簇索引
- 存储文件: InnoDB存储文件是frm、ibd，而MyISAM是frm、MYD、MYI
  - Innodb：frm是表定义文件，ibd是数据文件, 索引和数据在一个文件
  - MyISAM：frm是表定义文件，myd是数据文件，myi是索引文件, 索引和数据在不同的文件
- 崩溃恢复：MyISAM崩溃后发生损坏的概率比 InnoDB 高很多，而且恢复的速度也更慢。

## 九. 主从复制

### 原理

三个步骤, 主要涉及三个线程：binlog 线程、I/O 线程和 SQL 线程。

- **binlog 线程: ** 负责将主服务器上的数据更改写入二进制日志（`Binary log`）中。
- **I/O 线程: ** 负责从主服务器上读取二进制日志，并写入从服务器的中继日志（`Relay log`）。
- **SQL 线程: ** 负责读取中继日志，解析出主服务器已经执行的数据更改并在从服务器中重放（Replay）。

![image-20201026173920287](https://gitee.com/p8t/picbed/raw/master/imgs/20201026173921.png)

第一步，在主库上记录`Bin log`。在每次准备提交事务完成数据更新前，主库将数据更新的事件记录到`Bin log`中。MySQL会按事务提交的顺序而非每条语句的执行顺序来记录`Bin log`。在记录`Bin log`后，主库会告诉存储引擎可以提交事务了。

第二步，备库将主库的`Bin log`复制到其本地的`Relay log`中。首先，备库会启动一个工作线程，称为I/O线程，I/O线程跟主库建立一个普通的客户端连接，然后在主库上启动一个特殊的二进制转储（binlog dump）线程（该线程没有对应的SQL命令），这个二进制转储线程会读取主库上`Bin log`中的事件。它不会对事件进行轮询。如果该线程追赶上了主库，它将进入睡眠状态，直到主库发送信号量通知其有新的事件产生时才会被唤醒，备库I/O线程会将接收到的事件记录到`Relay log`中。

第三步，由备库的SQL线程执行来执行，该线程从`Relay log`中读取事件并在备库执行，从而实现备库数据的更新。当SQL线程追赶上I/O线程时， `Relay log`通常已经在系统缓存中，所以`Relay log`的开销很低。SQL线程执行的事件也可以通过配置选项来决定是否写入其自己的`Bin log`中。

上图可以递归下去: 从库生成`Bin log`, 给下一个从库复制

**具体复制方式**

- 语句复制: 相当于重新执行SQL. 有些场景不能正确复制, 比如有时间函数
- 行复制: 相当于直接复制记录. 对记录批量设置会很占空间(`Bin log`)

MySQL能够在这两种复制模式间动态切换。默认情况下使用的是基于语句的复制方式，但如
果发现语句无法被正确地复制，就切换到基于行的复制模式。

**延迟问题**

复制过程中, 前两步都是顺序读写日志文件, 速度较快. 而第三步涉及到写数据, 会有大量的随机IO, 这个过程又是单线程, 只能由`SQL Thread`执行, 因此, 某些情况会产生较大的延迟.

5.7引入了MTS(multi-threaded slave), 5.6也有并行复制, 但是只能基于库

![image-20201027153654951](https://gitee.com/p8t/picbed/raw/master/imgs/20201027153656.png)

并行可以减少延迟, 但是不能随机分发, 必须保证分发后执行结果一致性.

- 按库分发
- 按表分发
- 按行分发

**实现**

### TODO

### 读写分离

前提配置了主从复制

- 主负责写
- 从负责读

减少了读写锁的冲突

增加冗余, 提高可用性

读写分离虽然可以提升系统的吞吐量和可用性，但同时也带来了数据不一致的问题。 这包括多个主库之间的数据一致性，以及主库与从库之间的数据一致性的问题。

可以使用shardingsphere数据库中间件实现

**实现**

### TODO

## 十. 数据分片

> 来源: [shardingsphere](https://shardingsphere.apache.org/document/legacy/4.x/document/cn/features/sharding/)

### 背景

传统的将数据集中存储至单一数据节点的解决方案，在性能、可用性和运维成本这三方面已经难于满足互联网的海量数据场景。

从性能方面来说，由于关系型数据库大多采用B+树类型的索引，在数据量超过阈值的情况下，索引深度的增加也将使得磁盘访问的IO次数增加，进而导致查询性能的下降；同时，高并发访问请求也使得集中式数据库成为系统的最大瓶颈。

从可用性的方面来讲，服务化的无状态型，能够达到较小成本的随意扩容，这必然导致系统的最终压力都落在数据库之上。而单一的数据节点，或者简单的主从架构，已经越来越难以承担。数据库的可用性，已成为整个系统的关键。

从运维成本方面考虑，当一个数据库实例中的数据达到阈值以上，对于DBA的运维压力就会增大。数据备份和恢复的时间成本都将随着数据量的大小而愈发不可控。一般来讲，单一数据库实例的数据的阈值在1TB之内，是比较合理的范围。

在传统的关系型数据库无法满足互联网场景需要的情况下，将数据存储至原生支持分布式的NoSQL的尝试越来越多。 但NoSQL对SQL的不兼容性以及生态圈的不完善，使得它们在与关系型数据库的博弈中始终无法完成致命一击，而关系型数据库的地位却依然不可撼动。

数据分片指按照某个维度将存放在单一数据库中的数据分散地存放至多个数据库或表中以达到提升性能瓶颈以及可用性的效果。 数据分片的有效手段是对关系型数据库进行分库和分表。分库和分表均可以有效的避免由数据量超过可承受阈值而产生的查询瓶颈。 除此之外，分库还能够用于有效的分散对数据库单点的访问量；分表虽然无法缓解数据库压力，但却能够提供尽量将分布式事务转化为本地事务的可能，一旦涉及到跨库的更新操作，分布式事务往往会使问题变得复杂。 使用多主多从的分片方式，可以有效的避免数据单点，从而提升数据架构的可用性。

通过分库和分表进行数据的拆分来使得各个表的数据量保持在阈值以下，以及对流量进行疏导应对高访问量，是应对高并发和海量数据系统的有效手段。 数据分片的拆分方式又分为垂直分片和水平分片。

### 垂直切分

按照业务拆分的方式称为垂直分片，又称为纵向拆分，它的核心理念是专库专用。 在拆分之前，一个数据库由多个数据表构成，每个表对应着不同的业务。而拆分之后，则是按照业务将表进行归类，分布到不同的数据库中，从而将压力分散至不同的数据库。 下图展示了根据业务需要，将用户表和订单表垂直分片到不同的数据库的方案。

![vertical_sharding](https://gitee.com/p8t/picbed/raw/master/imgs/20201026220331.png)

垂直分片往往需要对架构和设计进行调整。通常来讲，是来不及应对互联网业务需求快速变化的；而且，它也并无法真正的解决单点瓶颈。 垂直拆分可以缓解数据量和访问量带来的问题，但无法根治。如果垂直拆分之后，表中的数据量依然超过单节点所能承载的阈值，则需要水平分片来进一步处理。

### 水平切分

水平分片又称为横向拆分。 相对于垂直分片，它不再将数据根据业务逻辑分类，而是通过某个字段（或某几个字段），根据某种规则将数据分散至多个库或表中，每个分片仅包含数据的一部分。 例如：根据主键分片，偶数主键的记录放入0库（或表），奇数主键的记录放入1库（或表），如下图所示。

![horizontal_sharding](https://gitee.com/p8t/picbed/raw/master/imgs/20201026220446.png)

水平分片从理论上突破了单机数据量处理的瓶颈，并且扩展相对自由，是分库分表的标准解决方案。

### 挑战

虽然数据分片解决了性能、可用性以及单点备份恢复等问题，但分布式的架构在获得了收益的同时，也引入了新的问题。

面对如此散乱的分库分表之后的数据，应用开发工程师和数据库管理员对数据库的操作变得异常繁重就是其中的重要挑战之一。他们需要知道数据需要从哪个具体的数据库的分表中获取。

另一个挑战则是，能够正确的运行在单节点数据库中的SQL，在分片之后的数据库中并不一定能够正确运行。例如，分表导致表名称的修改，或者分页、排序、聚合分组等操作的不正确处理。

跨库事务也是分布式的数据库集群要面对的棘手事情。 合理采用分表，可以在降低单表数据量的情况下，尽量使用本地事务，善于使用同库不同表可有效避免分布式事务带来的麻烦。 在不能避免跨库事务的场景，有些业务仍然需要保持事务的一致性。 而基于XA的分布式事务由于在并发度高的场景中性能无法满足需要，并未被互联网巨头大规模使用，他们大多采用最终一致性的柔性事务代替强一致事务。

### 问题

#### 跨库事务

使用基于XA的分布式事务解决

#### 跨节点连接查询

将原来的连接分解成多个单表查询，然后在用户程序中进行连接

#### ID唯一性

> [nick hao](https://www.cnblogs.com/haoxinyue/p/5208136.html)

##### 1. 数据库自增长序列或字段

最常见的方式。利用数据库，全数据库唯一。

优点：

1）简单，代码方便，性能可以接受。

2）数字ID天然排序，对分页或者需要排序的结果很有帮助。

缺点：

1）不同数据库语法和实现不同，数据库迁移的时候或多数据库版本支持的时候需要处理。

2）在单个数据库或读写分离或一主多从的情况下，只有一个主库可以生成。有单点故障的风险。

3）在性能达不到要求的情况下，比较难于扩展。

4）如果遇见多个系统需要合并或者涉及到数据迁移会相当痛苦。

5）分表分库的时候会有麻烦。

优化方案：

1）针对主库单点，如果有多个Master库，则每个Master库设置的起始数字不一样，步长一样，可以是Master的个数。比如：Master1 生成的是 1，4，7，10，Master2生成的是2,5,8,11 Master3生成的是 3,6,9,12。这样就可以有效生成集群中的唯一ID，也可以大大降低ID生成数据库操作的负载。

##### 2. UUID

常见的方式。可以利用数据库也可以利用程序生成，一般来说全球唯一。

优点：

1）简单，代码方便。

2）生成ID性能非常好，基本不会有性能问题。

3）全球唯一，在遇见数据迁移，系统数据合并，或者数据库变更等情况下，可以从容应对。

缺点：

1）没有排序，无法保证趋势递增。

2）UUID往往是使用字符串存储，查询的效率比较低。

3）存储空间比较大，如果是海量数据库，就需要考虑存储量的问题。

4）传输数据量大

5）不可读。

##### 3. Redis生成ID

当使用数据库来生成ID性能不够要求的时候，我们可以尝试使用Redis来生成ID。这主要依赖于Redis是单线程的，所以也可以用生成全局唯一的ID。可以用Redis的原子操作 INCR和INCRBY来实现。

可以使用Redis集群来获取更高的吞吐量。假如一个集群中有5台Redis。可以初始化每台Redis的值分别是1,2,3,4,5，然后步长都是5。各个Redis生成的ID为：

A：1,6,11,16,21

B：2,7,12,17,22

C：3,8,13,18,23

D：4,9,14,19,24

E：5,10,15,20,25

这个，随便负载到哪个机确定好，未来很难做修改。但是3-5台服务器基本能够满足器上，都可以获得不同的ID。但是步长和初始值一定需要事先需要了。使用Redis集群也可以方式单点故障的问题。

另外，比较适合使用Redis来生成每天从0开始的流水号。比如订单号=日期+当日自增长号。可以每天在Redis中生成一个Key，使用INCR进行累加。

优点：

1）不依赖于数据库，灵活方便，且性能优于数据库。

2）数字ID天然排序，对分页或者需要排序的结果很有帮助。

缺点：

1）如果系统中没有Redis，还需要引入新的组件，增加系统复杂度。

2）需要编码和配置的工作量比较大。

##### 4. Twitter的snowflake算法

snowflake是Twitter开源的分布式ID生成算法，结果是一个long型的ID。其核心思想是：使用41bit作为毫秒数，10bit作为机器的ID（5个bit是数据中心，5个bit的机器ID），12bit作为毫秒内的流水号（意味着每个节点在每毫秒可以产生 4096 个 ID），最后还有一个符号位，永远是0。

snowflake算法可以根据自身项目的需要进行一定的修改。比如估算未来的数据中心个数，每个数据中心的机器数以及统一毫秒可以能的并发数来调整在算法中所需要的bit数。

优点：

1）不依赖于数据库，灵活方便，且性能优于数据库。

2）ID按照时间在单机上是递增的。

缺点：

1）在单机上是递增的，但是由于涉及到分布式环境，每台机器上的时钟不可能完全同步，也许有时候也会出现不是全局递增的情况。

### 分片实施方案

> [美团](https://tech.meituan.com/2016/11/18/dianping-order-db-sharding.html)

1. 查询切分

将ID和库的Mapping关系记录在一个单独的库中。

![image-20201026223452093](https://gitee.com/p8t/picbed/raw/master/imgs/20201026223453.png)

优点：ID和库的Mapping算法可以随意更改。
缺点：引入额外的单点。

2. 范围切分

比如按照时间区间或ID区间来切分。

![image-20201026223924455](https://gitee.com/p8t/picbed/raw/master/imgs/20201026223925.png)

优点：单表大小可控，天然水平扩展。
缺点：无法解决集中写入瓶颈的问题。

3. Hash切分

一般采用Mod来切分

![image-20201026224944238](https://gitee.com/p8t/picbed/raw/master/imgs/20201026224945.png)